{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9855fd5-73c4-4ccf-b39f-29290e36abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertConfig\n",
    "from train_bert import *\n",
    "from BERT_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d94cc0d7-0b93-43cf-9207-be0c6d6447e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e5a394c-fa0c-4daf-a4c8-743199608cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    do_lower_case = True\n",
    "    folder_model = 'pretrain_uncased'\n",
    "\n",
    "    path_input_train_data = \"../dataset/data/train_test_origin_1k_dev.csv\"\n",
    "    path_input_test_data = \"../dataset/data/val_origin_1k.csv\"\n",
    "    path_input_validation_data = None\n",
    "\n",
    "    load_data_from_pt = False\n",
    "    path_pt_train_dataset = \"../dataset/torch_dataset/train_test_origin_1k_dev.pt\"\n",
    "    path_pt_test_dataset = \"../dataset/torch_dataset/val_origin_1k.pt\"\n",
    "    path_pt_validation_dataset = None\n",
    "\n",
    "    path_log_file = \"save_model/log_file.txt\"\n",
    "    output_dir = \"save_model/\"\n",
    "\n",
    "    max_seq_length = 400\n",
    "    max_query_length = 64\n",
    "\n",
    "    batch_size = 16\n",
    "\n",
    "    num_labels = 2\n",
    "    weight_class = [1, 1]\n",
    "\n",
    "    learning_rate = 1e-5\n",
    "    gradient_accumulation_steps = 5\n",
    "    weight_decay = 0.0\n",
    "    adam_epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "    warmup_steps = 0\n",
    "    use_pooler = True\n",
    "    output_hidden_states = True\n",
    "    # if use_pooler= False (mean concat 4 CLS in 4 last hidden_state BERT)\n",
    "    # you need to set output_hidden_states=True.\n",
    "\n",
    "    num_train_epochs = 5\n",
    "    save_steps = int(300 / gradient_accumulation_steps)\n",
    "\n",
    "    no_cuda = False\n",
    "    n_gpu = 1\n",
    "    device = \"cuda:0\"\n",
    "    seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ddfedc-c9ac-48d4-b16e-6bcc1656eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_bert(args):\n",
    "    # need remake config with device option for train with another cuda device\n",
    "    config = BertConfig.from_pretrained(args.folder_model)\n",
    "\n",
    "    config = config.to_dict()\n",
    "    config.update({\"device\": args.device})\n",
    "    config.update({\"use_pooler\": args.use_pooler})\n",
    "    config.update({\"weight_class\": args.weight_class})\n",
    "    config.update({\"output_hidden_states\": args.output_hidden_states})\n",
    "    config = BertConfig.from_dict(config)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.folder_model)\n",
    "    model = QaBERT.from_pretrained(\"bert-base-multilingual-uncased\", config=config)\n",
    "    model = model.to(args.device)\n",
    "    train_squad(args, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165033ea-763d-4623-84df-f790c5b80ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "train_model_bert(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e4cff-328b-4aed-b36a-59b56e6637c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
