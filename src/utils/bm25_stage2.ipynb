{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14207fae-ccf2-45e1-86c7-0241909b20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, pickle\n",
    "from rank_bm25 import BM25Okapi\n",
    "import argparse\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import regex as re\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, OkapiBM25Model\n",
    "from gensim.similarities import SparseMatrixSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3bc897-3048-450b-bc13-cd6c1d15e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, use_memory_fs=False, nb_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc18552-301c-49bb-a9e9-1adde554c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = pd.read_json(path_or_buf = \"./za-data/wikipedia_20220620_cleaned.jsonl\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce4b98-e839-4a79-b744-cc493eb92eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from glob import glob \n",
    "import re \n",
    "from nltk import word_tokenize as lib_tokenizer \n",
    "import string\n",
    "\n",
    "def post_process(x):\n",
    "    x = \" \".join(word_tokenize(strip_context(x))).strip()\n",
    "    x = x.replace(\"\\n\",\" \")\n",
    "    x = \"\".join([i for i in x if i not in string.punctuation])\n",
    "    x = \" \".join(x.split()[:128])\n",
    "    return x\n",
    "\n",
    "dict_map = dict({})  \n",
    "def word_tokenize(text): \n",
    "    global dict_map \n",
    "    words = text.split() \n",
    "    words_norm = [] \n",
    "    for w in words: \n",
    "        if dict_map.get(w, None) is None: \n",
    "            dict_map[w] = ' '.join(lib_tokenizer(w)).replace('``', '\"').replace(\"''\", '\"') \n",
    "        words_norm.append(dict_map[w]) \n",
    "    return words_norm \n",
    " \n",
    "def strip_answer_string(text): \n",
    "    text = text.strip() \n",
    "    while text[-1] in '.,/><;:\\'\"[]{}+=-_)(*&^!~`': \n",
    "        if text[0] != '(' and text[-1] == ')' and '(' in text: \n",
    "            break \n",
    "        if text[-1] == '\"' and text[0] != '\"' and text.count('\"') > 1: \n",
    "            break \n",
    "        text = text[:-1].strip() \n",
    "    while text[0] in '.,/><;:\\'\"[]{}+=-_)(*&^!~`': \n",
    "        if text[0] == '\"' and text[-1] != '\"' and text.count('\"') > 1: \n",
    "            break \n",
    "        text = text[1:].strip() \n",
    "    text = text.strip() \n",
    "    return text \n",
    " \n",
    "def strip_context(text): \n",
    "    text = text.replace('\\n', ' ') \n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "    text = text.strip() \n",
    "    return text\n",
    "\n",
    "def check_(x):\n",
    "    x = str(x).lower()\n",
    "    return (x.isnumeric() or \"ngày\" in x or \"tháng\" in x or \"năm\" in x)\n",
    "\n",
    "def find_candidate_ids(x, raw_answer=None, already_added=[], topk=50):\n",
    "    x = str(x)\n",
    "    query = post_process(x).lower().split()\n",
    "    tfidf_query = tfidf_model[dictionary.doc2bow(query)]\n",
    "    scores = bm25_index[tfidf_query]\n",
    "    top_n = list(np.argsort(scores)[::-1][:topk])\n",
    "    top_n = [i for i in top_n if i not in already_added]\n",
    "    # scores = list(scores[top_n])\n",
    "    if raw_answer is not None:\n",
    "        raw_answer = raw_answer.strip()\n",
    "        if raw_answer in entity_dict:\n",
    "            title = entity_dict[raw_answer].replace(\"wiki/\",\"\").replace(\"_\",\" \")\n",
    "            extra_id = title2idx.get(title, -1)\n",
    "            # print((raw_answer,title,extra_id, extra_id not in top_n))\n",
    "            if extra_id != -1 and extra_id not in top_n:\n",
    "                print(f\"Add extra id {extra_id} for {raw_answer}\")\n",
    "                top_n.append(extra_id)\n",
    "                top_n = list(set(top_n))\n",
    "    scores = scores[top_n]\n",
    "    return list(top_n), np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa3c88-702b-4c34-b6ee-505cc547876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki['title_lower'] = df_wiki['title'].apply(lambda x: x.lower()).parallel_apply(post_process)\n",
    "df_wiki['text_lower'] = df_wiki['text'].apply(lambda x: x.lower()).parallel_apply(post_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fceca6a-473f-4290-8303-af7bc25423e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "title2idx = dict([(x.strip(),y) for x,y in zip(df_wiki.title, df_wiki.index.values)])\n",
    "train = json.load(open(\"./za-data/zac2022_train_merged_final.json\"))\n",
    "entity_dict =  json.load(open(\"./processed/entities.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70236952-26e8-4343-833e-3214e6093275",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = [doc.split() for doc in df_wiki['text_lower']] #simple tokenier\n",
    "dictionary = Dictionary(corpus)\n",
    "bm25_model = OkapiBM25Model(dictionary=dictionary)\n",
    "bm25_corpus = bm25_model[list(map(dictionary.doc2bow, corpus))]\n",
    "bm25_index = SparseMatrixSimilarity(bm25_corpus, num_docs=len(corpus), num_terms=len(dictionary),normalize_queries=False, normalize_documents=False)\n",
    "tfidf_model = TfidfModel(dictionary=dictionary, smartirs='bnn')  # Enforce binary weighting of queries\n",
    "dictionary.save(\"./outputs/bm25_stage2/full_text/dict\")\n",
    "tfidf_model.save(\"./outputs/bm25_stage2/full_text/tfidf\")\n",
    "bm25_index.save(\"./outputs/bm25_stage2/full_text/bm25_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f35790-9437-4e2c-96a8-19d07fb46c9f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = [doc.split() for doc in df_wiki['title_lower']] #simple tokenier\n",
    "dictionary = Dictionary(corpus)\n",
    "bm25_model = OkapiBM25Model(dictionary=dictionary)\n",
    "bm25_corpus = bm25_model[list(map(dictionary.doc2bow, corpus))]\n",
    "bm25_index = SparseMatrixSimilarity(bm25_corpus, num_docs=len(corpus), num_terms=len(dictionary),normalize_queries=False, normalize_documents=False)\n",
    "tfidf_model = TfidfModel(dictionary=dictionary, smartirs='bnn')  # Enforce binary weighting of queries\n",
    "dictionary.save(\"./outputs/bm25_stage2/title/dict\")\n",
    "tfidf_model.save(\"./outputs/bm25_stage2/title/tfidf\")\n",
    "bm25_index.save(\"./outputs/bm25_stage2/title/bm25_index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khoint",
   "language": "python",
   "name": "khoint"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
