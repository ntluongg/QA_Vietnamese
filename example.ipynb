{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b845a5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'bayanpy', 'graph_tool', 'infomap', 'wurlitzer', 'leidenalg'}\n",
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'pyclustering', 'ASLPAw'}\n",
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'wurlitzer', 'infomap', 'leidenalg'}\n"
     ]
    }
   ],
   "source": [
    "from pairwise_model import *\n",
    "from text_utils import *\n",
    "import regex as re\n",
    "from bm25_utils import BM25Gensim\n",
    "from qa_model import *\n",
    "from qa_single import *\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b2e1723-4c54-4c3a-87cc-c715781f7419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f46791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki_windows = pd.read_csv(\"./data/wikipedia_20220620_cleaned_v2.csv\")\n",
    "df_wiki = pd.read_csv(\"./data/wikipedia_20220620_short.csv\")\n",
    "df_wiki.title = df_wiki.title.apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b737e29a-ae3a-4cde-bde5-800c6ae9f5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trang Chính</td>\n",
       "      <td>Trang Chính\\n\\n&lt;templatestyles src=\"Wiki2021/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Internet Society</td>\n",
       "      <td>Internet Society\\n\\nInternet Society hay ISOC ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tiếng Việt</td>\n",
       "      <td>Tiếng Việt\\n\\nTiếng Việt, cũng gọi là tiếng Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>Ohio\\n\\nOhio (viết tắt là OH, viết tắt cũ là O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>California\\n\\nCalifornia (phát âm như \"Ca-li-p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              title                                               text\n",
       "0       Trang Chính  Trang Chính\\n\\n<templatestyles src=\"Wiki2021/s...\n",
       "1  Internet Society  Internet Society\\n\\nInternet Society hay ISOC ...\n",
       "2        Tiếng Việt  Tiếng Việt\\n\\nTiếng Việt, cũng gọi là tiếng Vi...\n",
       "3              Ohio  Ohio\\n\\nOhio (viết tắt là OH, viết tắt cũ là O...\n",
       "4        California  California\\n\\nCalifornia (phát âm như \"Ca-li-p..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94147c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dict = json.load(open(\"./data/entities.json\"))\n",
    "new_dict = dict()\n",
    "for key, val in entity_dict.items():\n",
    "    val = val.replace(\"wiki/\", \"\").replace(\"_\", \" \")\n",
    "    entity_dict[key] = val\n",
    "    key = preprocess(key)\n",
    "    new_dict[key.lower()] = val\n",
    "entity_dict.update(new_dict)\n",
    "title2idx = dict([(x.strip(), y) for x, y in zip(df_wiki.title, df_wiki.index.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9289cae",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for './data/phobert_final'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './data/phobert_final' is the correct path to a directory containing all relevant files for a PhobertTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_model \u001b[38;5;241m=\u001b[39m QAModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/phobert_final\u001b[39m\u001b[38;5;124m\"\u001b[39m, entity_dict)\n\u001b[0;32m      2\u001b[0m pairwise_model_stage1 \u001b[38;5;241m=\u001b[39m PairwiseModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnguyenvulebinh/vi-mrc-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mhalf()\n\u001b[0;32m      3\u001b[0m pairwise_model_stage1\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/pairwise_v2.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m), strict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Documents\\zac2022-e2e-qa\\qa_single.py:14\u001b[0m, in \u001b[0;36mQAModel.__init__\u001b[1;34m(self, model_checkpoint, entity_dict, thr, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_checkpoint, entity_dict,\n\u001b[0;32m     12\u001b[0m              thr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28msuper\u001b[39m(QAModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 14\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_checkpoint, do_lower_case\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForQuestionAnswering\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_checkpoint)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion-answering\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     17\u001b[0m                        tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(device\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:787\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    785\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    786\u001b[0m         )\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2012\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2006\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   2007\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2008\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2009\u001b[0m     )\n\u001b[0;32m   2011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m-> 2012\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2013\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2014\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2015\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2016\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2017\u001b[0m     )\n\u001b[0;32m   2019\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2020\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for './data/phobert_final'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './data/phobert_final' is the correct path to a directory containing all relevant files for a PhobertTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "qa_model = QAModel(\"./data/phobert_final\", entity_dict)\n",
    "pairwise_model_stage1 = PairwiseModel(\"nguyenvulebinh/vi-mrc-base\").half()\n",
    "pairwise_model_stage1.load_state_dict(torch.load(\"./data/pairwise_v2.bin\"), strict = False)\n",
    "pairwise_model_stage1.eval()\n",
    "\n",
    "pairwise_model_stage2 = PairwiseModel(\"nguyenvulebinh/vi-mrc-base\").half()\n",
    "pairwise_model_stage2.load_state_dict(torch.load(\"./data/pairwise_stage2_seed0.bin\"), strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_model_stage1 = BM25Gensim(\"./data/bm25_stage1/\", entity_dict, title2idx)\n",
    "bm25_model_stage2_full = BM25Gensim(\"./data/bm25_stage2/full_text/\", entity_dict, title2idx)\n",
    "bm25_model_stage2_title = BM25Gensim(\"./data/bm25_stage2/title/\", entity_dict, title2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_e2e(question):\n",
    "    #Bm25 retrieval for top200 candidates\n",
    "    query = preprocess(question).lower()\n",
    "    top_n, bm25_scores = bm25_model_stage1.get_topk_stage1(query, topk=200)\n",
    "    titles = [preprocess(df_wiki_windows.title.values[i]) for i in top_n]\n",
    "    texts = [preprocess(df_wiki_windows.text.values[i]) for i in top_n]\n",
    "    \n",
    "    #Reranking with pairwise model for top10\n",
    "    question = preprocess(question)\n",
    "    ranking_preds = pairwise_model_stage1.stage1_ranking(question, texts)\n",
    "    ranking_scores = ranking_preds * bm25_scores\n",
    "    \n",
    "    #Question answering\n",
    "    best_idxs = np.argsort(ranking_scores)[-10:]\n",
    "    ranking_scores = np.array(ranking_scores)[best_idxs]\n",
    "    texts = np.array(texts)[best_idxs]\n",
    "    best_answer = qa_model(question, texts, ranking_scores)\n",
    "    if best_answer is None:\n",
    "        return \"Chịu\"\n",
    "    bm25_answer = preprocess(str(best_answer).lower(), max_length=128, remove_puncts=True)\n",
    "    \n",
    "    #Entity mapping\n",
    "    if not check_number(bm25_answer):\n",
    "        bm25_question = preprocess(str(question).lower(), max_length=128, remove_puncts=True)\n",
    "        bm25_question_answer = bm25_question + \" \" + bm25_answer\n",
    "        candidates, scores = bm25_model_stage2_title.get_topk_stage2(bm25_answer, raw_answer=best_answer)\n",
    "        titles = [df_wiki.title.values[i] for i in candidates]\n",
    "        texts = [df_wiki.text.values[i] for i in candidates]\n",
    "        ranking_preds = pairwise_model_stage2.stage2_ranking(question, best_answer, titles, texts)\n",
    "        if ranking_preds.max() >= 0.1:\n",
    "            final_answer = titles[ranking_preds.argmax()]\n",
    "        else:\n",
    "            candidates, scores = bm25_model_stage2_full.get_topk_stage2(bm25_question_answer)\n",
    "            titles = [df_wiki.title.values[i] for i in candidates] + titles\n",
    "            texts = [df_wiki.text.values[i] for i in candidates] + texts\n",
    "            ranking_preds = np.concatenate(\n",
    "                [pairwise_model_stage2.stage2_ranking(question, best_answer, titles, texts), ranking_preds])\n",
    "        final_answer = \"wiki/\"+titles[ranking_preds.argmax()].replace(\" \",\"_\")\n",
    "    else:\n",
    "        final_answer = bm25_answer.lower()\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4433257",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_answer_e2e(\"Ai là chủ tịch nước Việt Nam 2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ff01c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
